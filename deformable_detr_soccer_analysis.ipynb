{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4429,"status":"ok","timestamp":1731392453438,"user":{"displayName":"Sai P","userId":"15285502759182123980"},"user_tz":480},"id":"gTXznGqry5E2"},"outputs":[],"source":["%%capture\n","# connect google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# !pip install SoccerNet\n","!pip install lightning timm transformers torchmetrics"]},{"cell_type":"markdown","metadata":{"id":"9gBo9bdOFyJW"},"source":["## Custom Dataset Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AVVVHWUqFyJW","outputId":"798b74b9-c215-492d-b84b-9658bbd5aaa2"},"outputs":[],"source":["from multiprocessing import process\n","import torch\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from PIL import Image\n","import pandas as pd\n","import os\n","from transformers import AutoImageProcessor\n","import pdb\n","\n","\n","class SoccerNetDataset(Dataset):\n","    \"\"\"\n","    A dataset class for loading and preprocessing images from the SoccerNet dataset for object detection tasks.\n","\n","    Attributes:\n","        root (str): The root directory of the dataset (e.g., 'data/tracking/train').\n","        processor (callable, optional): A processor for preprocessing the images.\n","        data (list): A list to store the images and their corresponding annotations.\n","        labelsToId (dict): A dictionary mapping class labels to their respective IDs.\n","    \"\"\"\n","    def __init__(self, root, processor=None):\n","        \"\"\"\n","        Initializes the SoccerNetDataset with the specified root directory and optional processor.\n","\n","        Args:\n","            root (str): The root directory of the dataset.\n","            processor (callable, optional): A processor for preprocessing the images.\n","        \"\"\"\n","        self.root = root\n","        self.processor = processor\n","        self.data = []\n","        self.labelsToId = {\"player_team_left\": 0, \"player_team_right\": 1, \"ball\": 2, \"referee\": 3, \"goalkeeper_team_left\": 4, \"goalkeeper_team_right\": 5, \"other\":6}\n","        self.id_to_label = {v: k for k, v in self.labelsToId.items()}\n","        for folder in os.listdir(root):\n","            if os.path.isdir(os.path.join(root, folder)):\n","                idToLabelLocal = self._parse_labels(os.path.join(root, folder, \"gameinfo.ini\"))\n","                img_folder = os.path.join(root, folder, \"img1\")\n","                gt = pd.read_csv(os.path.join(root, folder, \"gt\", \"gt.txt\"), header=None)\n","                gt.columns = [\"frame\", \"class\", \"x\", \"y\", \"w\", \"h\"] + [f\"extra_{i}\" for i in range(4)]\n","                annotations = {}\n","                for _, row in gt.iterrows():\n","                    imgName = f\"{str(row['frame']).zfill(6)}.jpg\"\n","                    # img = Image.open(os.path.join(img_folder, imgName))\n","                    label = idToLabelLocal[str(row[\"class\"])]\n","                    # if annotations key is not present in annotations, add it\n","                    if imgName not in annotations:\n","                        annotations[imgName] = []\n","                    # do i need image_id in the annotations?\n","                    annotations[imgName].append({\n","                        \"bbox\": row[[\"x\", \"y\", \"w\", \"h\"]].tolist(),\n","                        \"bbox_mode\": 0,\n","                        \"category_id\": self.labelsToId[label],\n","                        \"iscrowd\": 0,\n","                        \"area\" : row[\"w\"] * row[\"h\"]\n","                    })\n","\n","                for imgName in os.listdir(img_folder):\n","                    image_id = int(folder.split('-')[1] + imgName.split('.')[0])\n","                    img_data = {\"id\": image_id,\n","                                \"img\": Image.open(os.path.join(img_folder, imgName))}\n","                    self.data.append((img_data, annotations[imgName]))\n","            # break\n","\n","\n","    def _parse_labels(self, filepath):\n","        \"\"\"\n","        Parses the gameinfo.ini file to map class IDs to labels.\n","\n","        Args:\n","            filepath (str): The path to the gameinfo.ini file.\n","\n","        Returns:\n","            dict: A dictionary mapping class IDs to labels.\n","        \"\"\"\n","        labels = {}\n","        with open(filepath, \"r\") as file:\n","            for line in file:\n","                if line.startswith(\"trackletID\"):\n","                    parts = line.split(\"=\")\n","                    class_id = parts[0].split(\"_\")[1]\n","                    label = parts[1].split(\";\")[0]\n","                    labels[class_id] = label.strip().replace(\" \", \"_\")\n","                    # bug in the labels, fix it\n","                    if labels[class_id] == \"goalkeepers_team_left\": labels[class_id] = \"goalkeeper_team_left\"\n","                    elif labels[class_id] == \"goalkeepers_team_right\": labels[class_id] = \"goalkeeper_team_right\"\n","        print(labels)\n","        return labels\n","\n","    def __len__(self):\n","        \"\"\"\n","        Returns the number of samples in the dataset.\n","\n","        Returns:\n","            int: The number of samples in the dataset.\n","        \"\"\"\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Returns the image and corresponding annotations for the specified index.\n","\n","        Args:\n","            idx (int): The index of the sample to retrieve.\n","\n","        Returns:\n","            tuple: A tuple containing the image and its annotations. If a processor is provided, the image is preprocessed before being returned.\n","            image is a tensor of shape (channels, height, width)\n","            annotations is a list of dictionaries containing the bounding box coordinates, category ID, and iscrowd flag for each object in the image\n","        \"\"\"\n","        img_data, annotations = self.data[idx]\n","\n","        # category_id is the index of the label in the list of labels\n","        target = {\n","            \"image_id\": img_data[\"id\"],\n","            \"annotations\": annotations\n","        }\n","        if self.processor is None:\n","            return img_data[\"img\"], target\n","        inputs = self.processor(images=img_data[\"img\"], annotations=target, return_tensors=\"pt\")\n","        pixel_values = inputs['pixel_values'].squeeze(0) # remove batch dimension\n","        labels = inputs['labels'][0] # remove batch dimension\n","        return pixel_values, labels\n","\n","\n","\n","processor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\n","train_dataset_full = SoccerNetDataset(\"/content/drive/MyDrive/deformable-detr-soccer-analysis/data/tracking/train\", processor=processor)\n","test_dataset = SoccerNetDataset(\"/content/drive/MyDrive/deformable-detr-soccer-analysis/data/tracking/test\", processor=processor)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hQTGmOQW71GF"},"outputs":[],"source":["# # select 25% of train_dataset\n","# train_size = int(0.25 * len(train_dataset_full))\n","# train_dataset, _ = random_split(train_dataset_full, [train_size, len(train_dataset_full) - train_size])\n","# split the dataset into training and validation sets stratified by class\n","\n","train_dataset = train_dataset_full\n","train_size = int(0.8 * len(train_dataset_full))\n","# val_size = len(train_dataset) - train_size\n","val_size = len(train_dataset) - train_size\n","train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n","\n","# print train_dataset size and val_dataset size\n","print(f\"train_dataset size: {len(train_dataset)}\")\n","print(f\"val_dataset size: {len(val_dataset)}\")\n","print(f\"test_dataset size: {len(test_dataset)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nf4bOdqCVZqe"},"outputs":[],"source":["def collate_fn(batch):\n","    pixel_values = [item[0] for item in batch]\n","    encoding = processor.pad(pixel_values, return_tensors='pt')\n","    labels = [item[1] for item in batch]\n","    batch = {\n","            'pixel_values': encoding['pixel_values'],\n","            'pixel_mask': encoding['pixel_mask'],\n","            'labels': labels\n","        }\n","    return batch\n","\n","# data loader for training and validation sets\n","train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True,collate_fn=collate_fn)\n","val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n","test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n","\n","\n","# visualize one image from the dataset with bounding boxes and labels\n","# also for each line of code, explain what it does\n","def view_labels(dataset, idx, predictions):\n","  import matplotlib.pyplot as plt\n","  import matplotlib.patches as patches\n","  if predictions is None:\n","    img, labels = train_dataset[idx]\n","    # why? because matplotlib expects channels last format but pytorch uses channels first format\n","    # meaning the image tensor has shape (channels, height, width) but matplotlib expects (height, width, channels)\n","    # so permute the dimensions to match the expected format\n","    plt.imshow(img.permute(1, 2, 0))\n","    ax = plt.gca() # why? to get the current axes of the plot to add patches to it later on for bounding boxes and labels in the image\n","    # axes are the subplots meaning the region of the image where the data is plotted\n","    # so to add bounding boxes and labels to the image, we need to get the current axes of the plot\n","    # so that we can add patches to it\n","    # plot the bounding boxes and labels\n","    for bbox, label in zip(labels[\"boxes\"], labels[\"class_labels\"]):\n","        # bbox is a tensor of shape (4,) containing the bounding box coordinates in (x, y, w, h) format and normalized to [0, 1] based on the image size\n","        # label is a tensor containing the class ID of the object\n","        # convert the bounding box coordinates to absolute values\n","        # convert bbox based on the image size\n","        bbox = [bbox[0]*img.shape[2], bbox[1]*img.shape[1], bbox[2]*img.shape[2], bbox[3]*img.shape[1]]\n","        #bbox[0] is center\n","        rect = patches.Rectangle(\n","            (bbox[0] - bbox[2] / 2, bbox[1] - bbox[3] / 2), bbox[2], bbox[3], linewidth=1, edgecolor=\"r\", facecolor=\"none\"\n","        )\n","        ax.add_patch(rect)\n","        ax.text(bbox[0], bbox[1], f\"{test_dataset.id_to_label[label.item()]}\", color=\"red\")\n","    plt.show()\n","  else:\n","    img, _ = train_dataset[idx]\n","    plt.imshow(img.permute(1, 2, 0))\n","    ax = plt.gca()\n","    for bbox, label in zip(predictions[idx][\"boxes\"], predictions[idx][\"labels\"]):\n","      bbox = [bbox[0]*img.shape[2], bbox[1]*img.shape[1], bbox[2]*img.shape[2], bbox[3]*img.shape[1]]\n","      rect = patches.Rectangle(\n","          (bbox[0] - bbox[2] / 2, bbox[1] - bbox[3] / 2), bbox[2], bbox[3], linewidth=1, edgecolor=\"g\", facecolor=\"none\"\n","      )\n","      ax.add_patch(rect)\n","      ax.text(bbox[0], bbox[1], f\"{test_dataset.id_to_label[label.item()]}\", color=\"green\")\n","    plt.show()\n","\n","view_labels(train_dataset, 1, None)"]},{"cell_type":"markdown","metadata":{"id":"apR6XeP8FyJX"},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MhKnjQTMFyJX"},"outputs":[],"source":["import torch\n","from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\n","\n","class DeformableDetrForObjectDetectionModule(torch.nn.Module):\n","    \"\"\"\n","    Deformable DETR model for object detection.\n","\n","    Attributes:\n","        processor (AutoImageProcessor): A processor for preprocessing the images.\n","        model (DeformableDetrForObjectDetection): A Deformable DETR model for object detection\n","    \"\"\"\n","    def __init__(self):\n","        super(DeformableDetrForObjectDetectionModule, self).__init__()\n","        self.processor = AutoImageProcessor.from_pretrained(\"SenseTime/deformable-detr\")\n","        self.model = DeformableDetrForObjectDetection.from_pretrained(\"SenseTime/deformable-detr\", num_labels=len(test_dataset.labelsToId), ignore_mismatched_sizes=True)\n","        self.model.config.num_classes = len(test_dataset.labelsToId)\n","\n","\n","    def forward(self, batch):\n","        \"\"\"\n","        Forward pass of the model.\n","\n","        Args:\n","            images (tensor): The input images in the shape of (batch_size, channels, height, width).\n","            targets (list): The target annotations for the images in COCO format.\n","            Each target is a dictionary containing the following keys:\n","            - \"image_id\" (int): The ID of the image.\n","            - \"annotations\" (list): A list of dictionaries containing the bounding box coordinates, category ID, and iscrowd flag for each object in the image.\n","        \"\"\"\n","        # return_tensors=\"pt\" returns the processed images as PyTorch tensors\n","        # inputs = self.processor(images=images, annotations=targets, return_tensors=\"pt\")\n","        # **inputs unpacks the dictionary into keyword arguments for the model which expects pixel_values and annotations\n","        # for example, if inputs = {\"pixel_values\": ..., \"annotations\": ...}, then **inputs is equivalent to model(pixel_values=..., annotations=...)\n","        outputs = self.model(**batch)\n","        return outputs"]},{"cell_type":"markdown","metadata":{"id":"A8aeeBh6FyJX"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cAK8Yq5yFyJY"},"outputs":[],"source":["#import mean average precision\n","from networkx import number_attracting_components\n","from torchmetrics.detection import MeanAveragePrecision\n","\n","torch.cuda.empty_cache()\n","\n","class Trainer:\n","    \"\"\"\n","    Trainer for training the Deformable DETR model for object detection.\n","\n","    Attributes:\n","        module (DeformableDetrForObjectDetectionModule): The Deformable DETR model for object detection.\n","        optimizer (torch.optim.Adam): The Adam optimizer for training the model.\n","        criterion (callable): The loss function for training the model.\n","    \"\"\"\n","    def __init__(self, module, optimizer, device):\n","        self.module = module\n","        self.optimizer = optimizer\n","        # self.criterion = criterion\n","        # self.compute_metric = compute_metric\n","        # move module to device\n","        self.device = device\n","        self.module.to(device)\n","\n","\n","    def train_epoch(self, train_loader):\n","        torch.cuda.empty_cache()\n","        self.module.train()\n","        total_loss = 0\n","\n","        for batch_idx, batch in enumerate(train_loader):\n","            # move batch to device\n","            # while pixel_values is a tensor, labels is a list of dictionaries for each image\n","            # each dictionary contains size, is_crowd, bbox of all, labels of all\n","            batch['pixel_values']  = batch['pixel_values'].to(self.device)\n","            batch['pixel_mask'] = batch['pixel_mask'].to(self.device)\n","            batch['labels'] = [{k: v.to(self.device) for k, v in t.items()} for t in batch['labels']]\n","\n","            outputs = self.module(batch)\n","\n","            self.optimizer.zero_grad()\n","            loss = outputs.loss\n","            loss.backward()\n","            self.optimizer.step()\n","            loss_dict = outputs.loss_dict\n","            total_loss += loss.item()\n","\n","            print(f\"batch: {batch_idx}, train_loss: {loss.item()}, train_loss_dict: {loss_dict}\")\n","\n","\n","        avg_loss = total_loss / len(train_loader)\n","        return avg_loss\n","\n","    def val_epoch(self, val_loader):\n","      torch.cuda.empty_cache()\n","      self.module.eval()\n","      with torch.no_grad():\n","        total_loss = 0\n","        metric = MeanAveragePrecision(box_format='cxcywh')\n","        all_preds = []\n","        all_targets = []\n","        for batch_idx, batch in enumerate(val_loader):\n","            # move batch to device\n","            batch['pixel_values']  = batch['pixel_values'].to(self.device)\n","            batch['pixel_mask'] = batch['pixel_mask'].to(self.device)\n","            batch['labels'] = [{k: v.to(self.device) for k, v in t.items()} for t in batch['labels']]\n","\n","\n","            outputs = self.module(batch)\n","\n","            # Assume the existence of self.module.model.config.num_classes and outputs/logits, etc.\n","\n","            # Extract loss and accumulate\n","            loss = outputs.loss\n","            loss_dict = outputs.loss_dict\n","            total_loss += loss.item()\n","\n","            # Get the mask to remove the no-object class\n","            num_classes = self.module.model.config.num_classes\n","            outputs_labels = outputs.logits.argmax(-1)\n","            mask = (outputs_labels != num_classes)  # Shape: (batch_size, num_boxes)\n","            # get the probabilities\n","            scores = outputs.logits.softmax(-1)[..., :-1]\n","            scores = torch.sum(scores, dim=-1)\n","\n","            # Initialize lists for preds and targets\n","            preds, targets = [], []\n","\n","            # Iterate through the batch\n","            for i in range(outputs.pred_boxes.size(0)):\n","                # Apply the mask directly to filter boxes and labels\n","                filtered_boxes = outputs.pred_boxes[i][mask[i]]\n","                filtered_labels = outputs_labels[i][mask[i]]\n","                filtered_scores = scores[i][mask[i]]\n","\n","                # Convert tensors to lists of dictionaries\n","                preds.append({\n","                    \"boxes\": filtered_boxes.cpu(),\n","                    \"labels\": filtered_labels.cpu(),\n","                    \"scores\": filtered_scores.cpu()\n","                })\n","\n","            # # Process the targets (assuming batch[\"labels\"] is a list of dictionaries)\n","            for img_labels in batch[\"labels\"]:\n","                targets.append({\"boxes\": img_labels[\"boxes\"].cpu(), \"labels\": img_labels[\"class_labels\"].cpu()})\n","\n","            # print('preds: {}'.format(preds))\n","            # print('targets: {}'.format(targets))\n","            # Update metric\n","            metric.update(preds, targets)\n","\n","            print(f\"batch: {batch_idx}, val_loss: {loss.item()}, val_loss_dict: {loss_dict}, map: {metric.compute()['map']}\")\n","            print(f\"preds: {preds}\")\n","            all_preds.extend(preds)\n","            all_targets.extend(targets)\n","            break\n","\n","        avg_loss = total_loss / len(val_loader)\n","        metric = metric.compute()\n","        return avg_loss, metric, all_preds\n","\n","def main():\n","    batch_size = 8\n","    learning_rate = 1e-5\n","    num_epochs = 5\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    module = DeformableDetrForObjectDetectionModule()\n","    optimizer = torch.optim.AdamW(module.model.parameters(), lr=learning_rate)\n","    trainer = Trainer(module, optimizer, device)\n","\n","\n","    for epoch in range(num_epochs):\n","        train_loss = trainer.train_epoch(train_dataloader)\n","        val_loss, val_metrics, val_preds = trainer.val_epoch(val_dataloader)\n","\n","        print(f'epoch: {epoch+1}/{num_epochs}, train_loss: {train_loss:.4f}, val_loss: {val_loss:.4f}, val_metric: {val_metrics}')\n","        view_labels(val_dataset, 1, val_preds)\n","\n","    # test on test_dataset and visualize\n","    test_loss, test_metrics, all_preds = trainer.val_epoch(test_dataloader)\n","    print(f'test_loss: {test_loss:.4f}, test_metric: {test_metrics}')\n","\n","    view_labels(test_dataset, 1, all_preds)\n","\n","main()"]},{"cell_type":"markdown","metadata":{"id":"nAki1eYiFyJY"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"Lo_LMLG5FyJY"},"source":["# Evaluation on Test"]},{"cell_type":"markdown","metadata":{"id":"5cbQE8zOFyJY"},"source":["# Visualization"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
