{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gTXznGqry5E2"},"outputs":[],"source":["%%capture\n","# connect google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# !pip install SoccerNet\n","!pip install lightning timm transformers torchmetrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PwdeurB3yRL5"},"outputs":[],"source":["# import SoccerNet\n","# from SoccerNet.Downloader import SoccerNetDownloader\n","# mySoccerNetDownloader=SoccerNetDownloader(LocalDirectory=\"/content/drive/MyDrive/deformable-detr-soccer-analysis/data\")\n","\n","# mySoccerNetDownloader.downloadDataTask(task=\"tracking\", split=[\"train\", \"test\", \"challenge\"])\n","\n","# mySoccerNetDownloader.downloadDataTask(task=\"tracking-2023\", split=[\"train\", \"test\", \"challenge\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f8zlGwSBZmqM"},"outputs":[],"source":["# from email.mime import image\n","# from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\n","# import torch\n","# from PIL import Image\n","# import requests\n","\n","# gdrive_dir = \"/content/drive/deformable-detr-soccer-analysis\"\n","# url = \"data/tracking/train/SNMOT-060/img1/000002.jpg\"\n","# # image = Image.open(requests.get(url, stream=True).raw)\n","# image = Image.open(url)\n","\n","# processor = AutoImageProcessor.from_pretrained(\"SenseTime/deformable-detr\")\n","# model = DeformableDetrForObjectDetection.from_pretrained(\"SenseTime/deformable-detr\")\n","\n","# inputs = processor(images=image, return_tensors=\"pt\")\n","# outputs = model(**inputs)\n","\n","# # convert outputs (bounding boxes and class logits) to COCO API\n","# # let's only keep detections with score > 0.7\n","# target_sizes = torch.tensor([image.size[::-1]])\n","# results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.3)[0]\n","\n","# for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n","#     box = [round(i, 2) for i in box.tolist()]\n","#     print(\n","#             f\"Detected {model.config.id2label[label.item()]} with confidence \"\n","#             f\"{round(score.item(), 3)} at location {box}\"\n","#     )\n","\n","# # plot the image and the bounding boxes\n","# # also explain what each command does\n","\n","# import matplotlib.pyplot as plt\n","# import matplotlib.patches as patches\n","\n","# plt.imshow(image)\n","# ax = plt.gca()\n","# # for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n","# #     box = [round(i, 2) for i in box.tolist()]\n","# #     rect = patches.Rectangle(\n","# #         (box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=1, edgecolor=\"r\", facecolor=\"none\"\n","# #     )\n","# #     ax.add_patch(rect)\n","# #     ax.text(box[0], box[1], f\"{model.config.id2label[label.item()]}: {round(score.item(), 3)}\", color=\"red\")\n","\n","\n","# # plot the same image with ground trur\n","# # gt is located in one directory above the image at gt/gt.txt\n","# # it is a csv with first column as frame number, second number as class id, next two numbers as bounding box coordinates and the next two as width and height, we can ignore rest of the columns\n","# # plot the ground truth bounding boxes in green\n","# # the label is located in gameinfo.ini in one directory above the image\n","# # it is in this format trackletID_1= player team left;10\n","# # so for class id 1, the label is player team left;10\n","\n","# import pandas as pd\n","\n","# gt = pd.read_csv(\"data/tracking/train/SNMOT-060/gt/gt.txt\", header=None)\n","# gt.columns = [\"frame\", \"class\", \"x\", \"y\", \"w\", \"h\"] + [f\"extra_{i}\" for i in range(4)]\n","\n","# gt = gt[gt[\"frame\"] == 2]\n","\n","# label = open(\"data/tracking/train/SNMOT-060/gameinfo.ini\").read().split(\"\\n\")\n","# label = [i for i in label if i.startswith(\"trackletID\")]\n","# label = [i.split(\"=\")[1] for i in label]\n","\n","# for _, row in gt.iterrows():\n","#     rect = patches.Rectangle(\n","#         (row[\"x\"], row[\"y\"]), row[\"w\"], row[\"h\"], linewidth=1, edgecolor=\"g\", facecolor=\"none\"\n","#     )\n","#     ax.add_patch(rect)\n","#     ax.text(row[\"x\"], row[\"y\"], f\"{label[row['class']-1]}\", color=\"green\")\n","# plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"9gBo9bdOFyJW"},"source":["## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":866,"referenced_widgets":["7190c686cb5d4414a31688aa82a28ecf","45ea0dbb79b94b4e974b5316a4fea247","698e056c36114c75be4a41822a1209ed","69aee62ef76f480284e0e9d0af1d6e6f","21ea6ecfba134c83ac665e0446952c83","061204be023546db83e4c91f1935973d","240b71cfc6774745b2b801deec4c4d74","1c3fbcfae626452eb6eb5bf425f9f06d","adf26cfa45374b428d4edb96d58733c5","33479bd6da9740e0be9c1d42c9e265d6","d6dad7edd12846429d51c9b561eb25ff"]},"executionInfo":{"elapsed":77832,"status":"ok","timestamp":1723934446416,"user":{"displayName":"Sai P","userId":"15285502759182123980"},"user_tz":420},"id":"AVVVHWUqFyJW","outputId":"85583bc0-5ff5-48d3-c77a-89821fd91cd0"},"outputs":[],"source":["from multiprocessing import process\n","import torch\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from PIL import Image\n","import pandas as pd\n","import os\n","from transformers import AutoImageProcessor\n","import pdb\n","\n","\n","class SoccerNetDataset(Dataset):\n","    \"\"\"\n","    A dataset class for loading and preprocessing images from the SoccerNet dataset for object detection tasks.\n","\n","    Attributes:\n","        root (str): The root directory of the dataset (e.g., 'data/tracking/train').\n","        processor (callable, optional): A processor for preprocessing the images.\n","        data (list): A list to store the images and their corresponding annotations.\n","        labelsToId (dict): A dictionary mapping class labels to their respective IDs.\n","    \"\"\"\n","    def __init__(self, root, processor=None):\n","        \"\"\"\n","        Initializes the SoccerNetDataset with the specified root directory and optional processor.\n","\n","        Args:\n","            root (str): The root directory of the dataset.\n","            processor (callable, optional): A processor for preprocessing the images.\n","        \"\"\"\n","        self.root = root\n","        self.processor = processor\n","        self.data = []\n","        self.labelsToId = {\"player_team_left\": 0, \"player_team_right\": 1, \"ball\": 2, \"referee\": 3, \"goalkeeper_team_left\": 4, \"goalkeeper_team_right\": 5, \"other\":6}\n","        self.id_to_label = {v: k for k, v in self.labelsToId.items()}\n","        for folder in os.listdir(root):\n","            if os.path.isdir(os.path.join(root, folder)):\n","                idToLabelLocal = self._parse_labels(os.path.join(root, folder, \"gameinfo.ini\"))\n","                img_folder = os.path.join(root, folder, \"img1\")\n","                gt = pd.read_csv(os.path.join(root, folder, \"gt\", \"gt.txt\"), header=None)\n","                gt.columns = [\"frame\", \"class\", \"x\", \"y\", \"w\", \"h\"] + [f\"extra_{i}\" for i in range(4)]\n","                annotations = {}\n","                for _, row in gt.iterrows():\n","                    imgName = f\"{str(row['frame']).zfill(6)}.jpg\"\n","                    # img = Image.open(os.path.join(img_folder, imgName))\n","                    label = idToLabelLocal[str(row[\"class\"])]\n","                    # if annotations key is not present in annotations, add it\n","                    if imgName not in annotations:\n","                        annotations[imgName] = []\n","                    # do i need image_id in the annotations?\n","                    annotations[imgName].append({\n","                        \"bbox\": row[[\"x\", \"y\", \"w\", \"h\"]].tolist(),\n","                        \"bbox_mode\": 0,\n","                        \"category_id\": self.labelsToId[label],\n","                        \"iscrowd\": 0,\n","                        \"area\" : row[\"w\"] * row[\"h\"]\n","                    })\n","\n","                for imgName in os.listdir(img_folder):\n","                    image_id = int(folder.split('-')[1] + imgName.split('.')[0])\n","                    img_data = {\"id\": image_id,\n","                                \"img\": Image.open(os.path.join(img_folder, imgName))}\n","                    self.data.append((img_data, annotations[imgName]))\n","            break\n","\n","\n","    def _parse_labels(self, filepath):\n","        \"\"\"\n","        Parses the gameinfo.ini file to map class IDs to labels.\n","\n","        Args:\n","            filepath (str): The path to the gameinfo.ini file.\n","\n","        Returns:\n","            dict: A dictionary mapping class IDs to labels.\n","        \"\"\"\n","        labels = {}\n","        with open(filepath, \"r\") as file:\n","            for line in file:\n","                if line.startswith(\"trackletID\"):\n","                    parts = line.split(\"=\")\n","                    class_id = parts[0].split(\"_\")[1]\n","                    label = parts[1].split(\";\")[0]\n","                    labels[class_id] = label.strip().replace(\" \", \"_\")\n","                    # bug in the labels, fix it\n","                    if labels[class_id] == \"goalkeepers_team_left\": labels[class_id] = \"goalkeeper_team_left\"\n","                    elif labels[class_id] == \"goalkeepers_team_right\": labels[class_id] = \"goalkeeper_team_right\"\n","        print(labels)\n","        return labels\n","\n","    def __len__(self):\n","        \"\"\"\n","        Returns the number of samples in the dataset.\n","\n","        Returns:\n","            int: The number of samples in the dataset.\n","        \"\"\"\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Returns the image and corresponding annotations for the specified index.\n","\n","        Args:\n","            idx (int): The index of the sample to retrieve.\n","\n","        Returns:\n","            tuple: A tuple containing the image and its annotations. If a processor is provided, the image is preprocessed before being returned.\n","            image is a tensor of shape (channels, height, width)\n","            annotations is a list of dictionaries containing the bounding box coordinates, category ID, and iscrowd flag for each object in the image\n","        \"\"\"\n","        img_data, annotations = self.data[idx]\n","\n","        # category_id is the index of the label in the list of labels\n","        target = {\n","            \"image_id\": img_data[\"id\"],\n","            \"annotations\": annotations\n","        }\n","        if self.processor is None:\n","            return img_data[\"img\"], target\n","        inputs = self.processor(images=img_data[\"img\"], annotations=target, return_tensors=\"pt\")\n","        pixel_values = inputs['pixel_values'].squeeze(0) # remove batch dimension\n","        labels = inputs['labels'][0] # remove batch dimension\n","        return pixel_values, labels\n","\n","def collate_fn(batch):\n","    pixel_values = [item[0] for item in batch]\n","    encoding = processor.pad(pixel_values, return_tensors='pt')\n","    labels = [item[1] for item in batch]\n","    batch = {\n","            'pixel_values': encoding['pixel_values'],\n","            'pixel_mask': encoding['pixel_mask'],\n","            'labels': labels\n","        }\n","    return batch\n","\n","processor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\n","train_dataset = SoccerNetDataset(\"/content/drive/MyDrive/deformable-detr-soccer-analysis/data/tracking/train\", processor=processor)\n","test_dataset = SoccerNetDataset(\"/content/drive/MyDrive/deformable-detr-soccer-analysis/data/tracking/test\", processor=processor)\n","\n","# split the dataset into training and validation sets stratified by class\n","train_size = int(0.8 * len(train_dataset))\n","val_size = len(train_dataset) - train_size\n","train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n","\n","# data loader for training and validation sets\n","train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True,collate_fn=collate_fn)\n","val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n","test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n","\n","\n","# visualize one image from the dataset with bounding boxes and labels\n","# also for each line of code, explain what it does\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","\n","img, labels = train_dataset[100]\n","# why? because matplotlib expects channels last format but pytorch uses channels first format\n","# meaning the image tensor has shape (channels, height, width) but matplotlib expects (height, width, channels)\n","# so permute the dimensions to match the expected format\n","plt.imshow(img.permute(1, 2, 0))\n","ax = plt.gca() # why? to get the current axes of the plot to add patches to it later on for bounding boxes and labels in the image\n","# axes are the subplots meaning the region of the image where the data is plotted\n","# so to add bounding boxes and labels to the image, we need to get the current axes of the plot\n","# so that we can add patches to it\n","# plot the bounding boxes and labels\n","print(labels)\n","for bbox, label in zip(labels[\"boxes\"], labels[\"class_labels\"]):\n","    # bbox is a tensor of shape (4,) containing the bounding box coordinates in (x, y, w, h) format and normalized to [0, 1] based on the image size\n","    # label is a tensor containing the class ID of the object\n","    # convert the bounding box coordinates to absolute values\n","    # convert bbox based on the image size\n","    bbox = [bbox[0]*img.shape[2], bbox[1]*img.shape[1], bbox[2]*img.shape[2], bbox[3]*img.shape[1]]\n","    #bbox[0] is center\n","    rect = patches.Rectangle(\n","        (bbox[0] - bbox[2] / 2, bbox[1] - bbox[3] / 2), bbox[2], bbox[3], linewidth=1, edgecolor=\"r\", facecolor=\"none\"\n","    )\n","    ax.add_patch(rect)\n","    ax.text(bbox[0], bbox[1], f\"{test_dataset.id_to_label[label.item()]}\", color=\"red\")\n","plt.show()\n","# for annotation in target[\"annotations\"]:\n","#     bbox = annotation[\"bbox\"]\n","#     category_id = annotation[\"category_id\"]\n","#     # convert category_id to label\n","#     label = list(train_dataset.labelsToId.keys())[category_id]\n","#     rect = patches.Rectangle(\n","#         (bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=1, edgecolor=\"r\", facecolor=\"none\"\n","#     )\n","#     ax.add_patch(rect)\n","#     ax.text(bbox[0], bbox[1], label, color=\"red\")\n","# plt.show()"]},{"cell_type":"markdown","metadata":{"id":"apR6XeP8FyJX"},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MhKnjQTMFyJX"},"outputs":[],"source":["# create a model using torch.nn.Module for object detection\n","# it should have all the necessary components for object detection\n","# like model, optimizer, loss function, forward method,\n","# the model should be DeformableDetrForObjectDetection from transformers\n","# load the pretrained weights from SenseTime/deformable-detr\n","# use the AutoImageProcessor to preprocess the images\n","# use the Adam optimizer with a fine-tune learning rate of 1e-5\n","\n","import torch\n","from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\n","\n","class DeformableDetrForObjectDetectionModule(torch.nn.Module):\n","    \"\"\"\n","    Deformable DETR model for object detection.\n","\n","    Attributes:\n","        processor (AutoImageProcessor): A processor for preprocessing the images.\n","        model (DeformableDetrForObjectDetection): A Deformable DETR model for object detection\n","    \"\"\"\n","    def __init__(self):\n","        super(DeformableDetrForObjectDetectionModule, self).__init__()\n","        self.processor = AutoImageProcessor.from_pretrained(\"SenseTime/deformable-detr\")\n","        self.model = DeformableDetrForObjectDetection.from_pretrained(\"SenseTime/deformable-detr\")\n","        # change the number of classes in the model to match the number of classes in the dataset\n","        self.model.config.num_classes = len(train_dataset.labelsToId)\n","\n","    def forward(self, batch):\n","        \"\"\"\n","        Forward pass of the model.\n","\n","        Args:\n","            images (tensor): The input images in the shape of (batch_size, channels, height, width).\n","            targets (list): The target annotations for the images in COCO format.\n","            Each target is a dictionary containing the following keys:\n","            - \"image_id\" (int): The ID of the image.\n","            - \"annotations\" (list): A list of dictionaries containing the bounding box coordinates, category ID, and iscrowd flag for each object in the image.\n","        \"\"\"\n","        # return_tensors=\"pt\" returns the processed images as PyTorch tensors\n","        # inputs = self.processor(images=images, annotations=targets, return_tensors=\"pt\")\n","        # **inputs unpacks the dictionary into keyword arguments for the model which expects pixel_values and annotations\n","        # for example, if inputs = {\"pixel_values\": ..., \"annotations\": ...}, then **inputs is equivalent to model(pixel_values=..., annotations=...)\n","        outputs = self.model(**batch)\n","        return outputs"]},{"cell_type":"markdown","metadata":{"id":"A8aeeBh6FyJX"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"executionInfo":{"elapsed":319,"status":"error","timestamp":1723429944926,"user":{"displayName":"Sai P","userId":"15285502759182123980"},"user_tz":420},"id":"cAK8Yq5yFyJY","outputId":"3ad3e237-d817-4d8e-ee34-95bee8e248dc"},"outputs":[],"source":["#import mean average precision\n","from networkx import number_attracting_components\n","from torchmetrics.detection import MeanAveragePrecision\n","\n","class Trainer:\n","    \"\"\"\n","    Trainer for training the Deformable DETR model for object detection.\n","\n","    Attributes:\n","        module (DeformableDetrForObjectDetectionModule): The Deformable DETR model for object detection.\n","        optimizer (torch.optim.Adam): The Adam optimizer for training the model.\n","        criterion (callable): The loss function for training the model.\n","    \"\"\"\n","    def __init__(self, module, optimizer, device):\n","        self.module = module\n","        self.optimizer = optimizer\n","        # self.criterion = criterion\n","        # self.compute_metric = compute_metric\n","        # move module to device\n","        self.device = device\n","        self.module.to(device)\n","\n","\n","    def train_epoch(self, train_loader):\n","        self.module.train()\n","        total_loss = 0\n","\n","        for batch_idx, batch in enumerate(train_loader):\n","            # move batch to device\n","            batch = {k: v.to(self.device) for k, v in batch.items()}\n","\n","            outputs = self.module(batch)\n","\n","            self.optimizer.zero_grad()\n","            self.optimizer.step()\n","\n","            loss = outputs.loss\n","            loss_dict = outputs.loss_dict\n","            total_loss += loss.item()\n","\n","            print(f\"batch: {batch_idx}, train_loss: {loss.item()}, train_loss_dict: {loss_dict}\")\n","\n","\n","        avg_loss = total_loss / len(train_loader)\n","        return avg_loss\n","\n","    def val_epoch(self, val_loader):\n","        self.module.eval()\n","        total_loss = 0\n","        metric = MeanAveragePrecision()\n","\n","        for batch_idx, batch in enumerate(val_loader):\n","            # move batch to device\n","            batch = {k: v.to(self.device) for k, v in batch.items()}\n","\n","            outputs = self.module(batch)\n","\n","            # Assume the existence of self.module.model.config.num_classes and outputs/logits, etc.\n","\n","            # Extract loss and accumulate\n","            loss = outputs.loss\n","            loss_dict = outputs.loss_dict()\n","            total_loss += loss.item()\n","\n","            # Get the mask to remove the no-object class\n","            num_classes = self.module.model.config.num_classes\n","            outputs_labels = outputs.logits.argmax(-1)\n","            mask = (outputs_labels != num_classes)  # Shape: (batch_size, num_boxes)\n","\n","            # Initialize lists for preds and targets\n","            preds, targets = [], []\n","\n","            # Iterate through the batch\n","            for i in range(outputs.pred_boxes.size(0)):\n","                # Apply the mask directly to filter boxes and labels\n","                filtered_boxes = outputs.pred_boxes[i][mask[i]]\n","                filtered_labels = outputs_labels[i][mask[i]]\n","\n","                # Convert tensors to lists of dictionaries\n","                preds.append({\n","                    \"boxes\": filtered_boxes.cpu().tolist(),\n","                    \"labels\": filtered_labels.cpu().tolist(),\n","                })\n","\n","            # Process the targets (assuming batch[\"labels\"] is a list of dictionaries)\n","            for img_labels in batch[\"labels\"]:\n","                boxes = [label_dict[\"bbox\"] for label_dict in img_labels]\n","                labels = [label_dict[\"label\"] for label_dict in img_labels]\n","                targets.append({\"boxes\": boxes, \"labels\": labels})\n","\n","            # Update metric\n","            metric.update(preds, targets)\n","\n","            print(f\"batch: {batch_idx}, val_loss: {loss.item()}, val_loss_dict: {loss_dict}, map: {metric}\")\n","\n","\n","        avg_loss = total_loss / len(val_loader)\n","        metric = metric.compute()\n","        return avg_loss, metric\n","\n","def main():\n","    batch_size = 8\n","    learning_rate = 1e-5\n","    num_epochs = 20\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    module = DeformableDetrForObjectDetectionModule()\n","    optimizer = torch.optim.AdamW(module.model.parameters(), lr=learning_rate)\n","    trainer = Trainer(module, optimizer, device)\n","\n","\n","    for epoch in range(num_epochs):\n","        train_loss = trainer.train_epoch(train_dataloader)\n","        val_loss, val_metrics = trainer.val_epoch(val_dataloader)\n","\n","    print(f'epoch: {epoch+1}/{num_epochs}, train_loss: {train_loss:.4f}, val_loss: {val_loss:.4f}, val_metric: {val_metrics}')\n","\n","main()"]},{"cell_type":"markdown","metadata":{"id":"nAki1eYiFyJY"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"Lo_LMLG5FyJY"},"source":["# Evaluation on Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xl9Gw1cgFyJY"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"5cbQE8zOFyJY"},"source":["# Visualization"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"061204be023546db83e4c91f1935973d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c3fbcfae626452eb6eb5bf425f9f06d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21ea6ecfba134c83ac665e0446952c83":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"240b71cfc6774745b2b801deec4c4d74":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33479bd6da9740e0be9c1d42c9e265d6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45ea0dbb79b94b4e974b5316a4fea247":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_061204be023546db83e4c91f1935973d","placeholder":"​","style":"IPY_MODEL_240b71cfc6774745b2b801deec4c4d74","value":"preprocessor_config.json: 100%"}},"698e056c36114c75be4a41822a1209ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c3fbcfae626452eb6eb5bf425f9f06d","max":305,"min":0,"orientation":"horizontal","style":"IPY_MODEL_adf26cfa45374b428d4edb96d58733c5","value":305}},"69aee62ef76f480284e0e9d0af1d6e6f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33479bd6da9740e0be9c1d42c9e265d6","placeholder":"​","style":"IPY_MODEL_d6dad7edd12846429d51c9b561eb25ff","value":" 305/305 [00:00&lt;00:00, 21.4kB/s]"}},"7190c686cb5d4414a31688aa82a28ecf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_45ea0dbb79b94b4e974b5316a4fea247","IPY_MODEL_698e056c36114c75be4a41822a1209ed","IPY_MODEL_69aee62ef76f480284e0e9d0af1d6e6f"],"layout":"IPY_MODEL_21ea6ecfba134c83ac665e0446952c83"}},"adf26cfa45374b428d4edb96d58733c5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d6dad7edd12846429d51c9b561eb25ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
